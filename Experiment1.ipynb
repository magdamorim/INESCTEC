{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWvmULpRY4RmriNH2K7u3w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magdamorim/INESCTEC/blob/main/Experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "xX_qOUb9wgRJ"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel,GPT2Tokenizer, GPT2Model\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Fashion MNIST dataset\n",
        "dataset = load_dataset(\"fashion_mnist\")"
      ],
      "metadata": {
        "id": "NonDOtubxHqC"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=[\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "# Function to plot an image\n",
        "def show_image(image, label):\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(f\"Label:{label} : {classes[label]}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Load an example image and label\n",
        "example = dataset['train'][100]\n",
        "image = example['image']\n",
        "label = example['label']\n",
        "\n",
        "# Display the image\n",
        "show_image(image, label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "J0hPuLJoxXfI",
        "outputId": "15b1233c-0a7c-40d5-96ac-c06feed4fa8d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download models pretrained"
      ],
      "metadata": {
        "id": "nyDLQ26gClzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the CLIP model with ViT-B-32. # CLIP\n",
        "model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# Initialize the ViT processor and model # IMAGE ENCODER\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model_image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "\n",
        "# Load the GPT-2 model and tokenizer TEXT ENCODER\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # breaks text into tokens and gives qual attention\n",
        "gpt_model = GPT2Model.from_pretrained(\"gpt2\") # pretreained model\n",
        "\n"
      ],
      "metadata": {
        "id": "jzm3MY0PxbV-"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "furbuy2kCdEr"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer splits the text into tokens and gives an ID according to vocabulary. Example 32 means start sequence. Then it gives a constant attention mask that changes when applying the model to give attention to certain words."
      ],
      "metadata": {
        "id": "J1hY_IBy6Xhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEXT ENCODER"
      ],
      "metadata": {
        "id": "qcKgnYRrBpdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"A stylish shirt for a casual look.\"\n",
        "text=classes[label]\n",
        "\n",
        "\n",
        "inputs = gpt_tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model\n",
        "# disable the gradient because is zero shoot training so just run model without training\n",
        "# it allows faster computation without gradient\n",
        "with torch.no_grad():\n",
        "    outputs = gpt_model(**inputs)\n",
        "\n",
        "# Extract the features (hidden states)\n",
        "# outputs have everything like attention scores, etc.\n",
        "features = outputs.last_hidden_state\n"
      ],
      "metadata": {
        "id": "71rgAdYqz4CY"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"attention_mask\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c9hK7qY0GuF",
        "outputId": "861703c6-8930-4d3d-ab97-81df46f86f6a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p68jlfH79B1W",
        "outputId": "f5ce26d3-a28b-47b2-8719-ef218f0c4b07"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.1427, -0.0256, -0.2602,  ..., -0.1504, -0.0064, -0.0070],\n",
              "         [ 0.4898, -0.7599,  0.0847,  ..., -0.0113, -0.5079, -0.2114]]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gpt_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NghI2M-5NwC",
        "outputId": "37fcd677-3515-461c-9f30-c370e79b6732"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-11): 12 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2Attention(\n",
            "        (c_attn): Conv1D()\n",
            "        (c_proj): Conv1D()\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D()\n",
            "        (c_proj): Conv1D()\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IMAGE ENCODER"
      ],
      "metadata": {
        "id": "5CETAspOBm2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example preprocessing function for images\n",
        "def preprocess_image(image_array):\n",
        "    # Convert numpy array to PIL image\n",
        "    image = Image.fromarray(np.uint8(image_array), mode=\"L\").convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    return inputs\n",
        "\n",
        "# Preprocess the first image in the dataset\n",
        "image = dataset['train'][0]['image']\n",
        "image_inputs = preprocess_image(image)\n",
        "\n",
        "# Forward pass through the ViT model\n",
        "with torch.no_grad():\n",
        "    outputs = model_image_encoder(**image_inputs)\n",
        "\n",
        "# Extract the features from the last hidden state\n",
        "image_features = outputs.last_hidden_state\n",
        "\n",
        "# Now you have the image features\n",
        "print(image_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-pv2SXD70pl",
        "outputId": "6b5a5812-efe0-40c3-fa6a-7c0abfa2130e"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.2964,  0.1555, -0.1476,  ..., -0.0137, -0.1118, -0.0617],\n",
            "         [ 0.2291,  0.1328, -0.1272,  ..., -0.1577,  0.0556, -0.0293],\n",
            "         [ 0.0217,  0.1546, -0.1513,  ..., -0.2677,  0.2969,  0.0590],\n",
            "         ...,\n",
            "         [ 0.1883,  0.1592, -0.1517,  ..., -0.2992,  0.2873, -0.0099],\n",
            "         [ 0.1598,  0.1350, -0.0992,  ..., -0.2464,  0.3216, -0.0131],\n",
            "         [ 0.2415,  0.0390, -0.0826,  ..., -0.1469,  0.3982,  0.0012]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K42RQg2-CxQv"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}